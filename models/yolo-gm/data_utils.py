from pathlib import Path
import textwrap

import kagglehub
import yaml

# Download the data rupankarmajumdar/crop-pests-dataset using kagglehub.
def download_crop_pests_dataset() -> Path:
    print("[data_utils] Downloading Kaggle dataset rupankarmajumdar/crop-pests-dataset ...")
    path_str = kagglehub.dataset_download("rupankarmajumdar/crop-pests-dataset")
    path = Path(path_str)
    print(f"[data_utils] Dataset downloaded to: {path}")
    return path

# Try to read class names from YAML files within the dataset.
def _infer_class_names_from_yaml(dataset_root: Path):
    candidate_names = ["data.yaml", "dataset.yaml", "AgroPest.yaml", "AgroPest-12.yaml"]
    for fname in candidate_names:
        ypath = dataset_root / fname
        if ypath.exists():
            try:
                data = yaml.safe_load(ypath.read_text(encoding="utf-8"))
                names = data.get("names", None)
                if isinstance(names, dict):
                    # Example format: {0: 'Ants', 1: 'Bees', ...}
                    names = [names[k] for k in sorted(names.keys(), key=lambda x: int(x))]
                if isinstance(names, list) and len(names) > 0:
                    print(f"[data_utils] Found class names from {fname}: {names}")
                    return names
            except Exception as e:
                print(f"[data_utils] Failed to parse {fname}: {e}")
    return None

# Try to read class names from classes.txt / obj.names type files.
def _infer_class_names_from_txt(dataset_root: Path):
    candidate_files = ["classes.txt", "classes.names", "obj.names"]
    for fname in candidate_files:
        tpath = dataset_root / fname
        if tpath.exists():
            try:
                lines = [
                    line.strip()
                    for line in tpath.read_text(encoding="utf-8").splitlines()
                    if line.strip()
                ]
                if lines:
                    print(f"[data_utils] Found class names from {fname}: {lines}")
                    return lines
            except Exception as e:
                print(f"[data_utils] Failed to read {fname}: {e}")
    return None

# If no name files are found, scan train/labels/*.txt
# Determine the max class id and generate ['class_0', 'class_1', ...]
def _infer_class_names_from_labels(dataset_root: Path):
    labels_dir = dataset_root / "train" / "labels"
    if not labels_dir.exists():
        return None

    max_id = -1
    for txt in labels_dir.rglob("*.txt"):
        try:
            for line in txt.read_text(encoding="utf-8").splitlines():
                line = line.strip()
                if not line:
                    continue
                # YOLO label format: class_id cx cy w h ...
                first = line.split()[0]
                cid = int(first)
                if cid > max_id:
                    max_id = cid
        except Exception as e:
            print(f"[data_utils] Failed to parse label file {txt}: {e}")

    if max_id >= 0:
        names = [f"class_{i}" for i in range(max_id + 1)]
        print(f"[data_utils] Inferred {len(names)} classes from labels: {names}")
        return names

    return None

'''
Combine all methods above to automatically infer class names.
Priority:
    1) From data.yaml / dataset.yaml
    2) From classes.txt / obj.names
    3) From class ids in train/labels -> class_0, class_1, ...
'''
def infer_class_names(dataset_root: Path):
    dataset_root = dataset_root.resolve()

    names = _infer_class_names_from_yaml(dataset_root)
    if names:
        return names

    names = _infer_class_names_from_txt(dataset_root)
    if names:
        return names

    names = _infer_class_names_from_labels(dataset_root)
    if names:
        return names

    print("[data_utils] Unable to infer class names from dataset. Using default ['class_0']")
    return ["class_0"]

"""
Generate pest.yaml under project_root, pointing to train/valid/test directories.

Expected dataset structure:
    dataset_root/
        train/images
        train/labels
        valid/images
        valid/labels
        test/images
        test/labels
"""
def make_pest_yaml(project_root: Path,
                   dataset_root: Path,
                   yaml_name: str = "pest.yaml") -> str:

    project_root = project_root.resolve()
    dataset_root = dataset_root.resolve()

    train_images = dataset_root / "train" / "images"
    val_images = dataset_root / "valid" / "images"
    test_images = dataset_root / "test" / "images"

    class_names = infer_class_names(dataset_root)
    nc = len(class_names)

    yaml_path = project_root / yaml_name

    yaml_content = textwrap.dedent(f"""
    # Auto-generated dataset config for YOLO
    # Original dataset: Kaggle - rupankarmajumdar/crop-pests-dataset
    # This file is automatically generated by data_utils.make_pest_yaml

    path: {dataset_root}

    train: {train_images}
    val: {val_images}
    test: {test_images}

    nc: {nc}  # number of classes
    names: {class_names}
    """).strip() + "\n"

    yaml_path.write_text(yaml_content, encoding="utf-8")
    print(f"[data_utils] Generated dataset config file: {yaml_path}")
    return str(yaml_path)

"""
If pest.yaml already exists under project_root, use it directly;
otherwise:
    1) Download Kaggle dataset
    2) Auto-generate pest.yaml (with inferred class names)
Returns: absolute path (str) to pest.yaml
"""
def get_or_create_pest_yaml(project_root: Path,
                            yaml_name: str = "pest.yaml") -> str:
    project_root = project_root.resolve()
    yaml_path = project_root / yaml_name

    if yaml_path.exists():
        print(f"[data_utils] Found existing {yaml_name}, using: {yaml_path}")
        return str(yaml_path)

    dataset_root = download_crop_pests_dataset()
    return make_pest_yaml(project_root, dataset_root, yaml_name=yaml_name)
